
-- How does SpatialSAD work?
It uses some nice tricks to optimize the operation of large convolution-like operations.
You can use the same tricks to do VERY optimized functions that work in the ‘nn’ torch package!

-- Convolutional version suggested by Clement July31st2012:   
-- assuming template is 3D (color template) NTx3(colors)xTHxTW
-- and input is also 3D, but w/ larger width and height 3(colors)xHxW

—- create example input and templates (SAD kernels):

t7> input = torch.ceil(torch.abs(torch.randn(1,3,3)*4))
t7> return input
(1,.,.) = 
  5  4  2
  7  3  4
  2  2  4
[torch.DoubleTensor of dimension 1x3x3]

t7> templates = torch.ceil(torch.abs(torch.randn(2,1,2,2)*4))
t7> =templates
(1,1,.,.) = 
  4  1
  3  8

(2,1,.,.) = 
  1  2
  4  2
[torch.DoubleTensor of dimension 2x1x2x2]

t7> input_unfold = input:unfold(2,templates:size(3),1):unfold(3,templates:size(4),1)
t7> return input_unfold
(1,1,1,.,.) = 
  5  4
  7  3
(1,2,1,.,.) = 
  7  3
  2  2
(1,1,2,.,.) = 
  4  2
  3  4
(1,2,2,.,.) = 
  3  4
  2  4
[torch.DoubleTensor of dimension 1x2x2x2x2]

-- input_unfold is now 3x(H-TH+1)x(W-TW+1)xTHxTW with replicated input patches aligned for the template operations

t7> templates_rep = templates[1]:clone():resize(templates:size(2), 1, 1, templates:size(3), templates:size(4))
t7> =templates_rep
(1,1,1,.,.) = 
  4  1
  3  8
[torch.DoubleTensor of dimension 1x1x1x2x2]

t7> templates_rep = templates_rep:expandAs(input_unfold)
t7> return templates_rep
(1,1,1,.,.) = 
  4  1
  3  8
(1,2,1,.,.) = 
  4  1
  3  8
(1,1,2,.,.) = 
  4  1
  3  8
(1,2,2,.,.) = 
  4  1
  3  8
[torch.DoubleTensor of dimension 1x2x2x2x2]

-- template_rep is now also 3x(H-TH+1)x(W-TW+1)xTHxTW, with the same values replicated, but still of the same size in memory!
-- it just got expanded to have the same size as input_unfold

—- now this is the math step: compute 1-norm, or SAD, or sum of absolute values of the input with the template:
-- this is done for each location of the input image here in one step!!!!

t7> L1s = input_unfold:clone():add(-1,templates_rep):abs():sum(5):sum(4):sum(1):squeeze()
t7> =L1s
 13   5
 12   9
[torch.DoubleTensor of dimension 2x2]

-- L1s is now a (H-TH+1)x(W-TW+1) tensor, where each entry is the L1 for the given location
—- this is the output of the desired operation!









